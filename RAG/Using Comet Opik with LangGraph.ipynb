{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "print(os.getenv(\"OPENAI_API_KEY\")[:20])\n",
    "print(os.getenv(\"OPIK_WORKSPACE\"))\n",
    "print(os.getenv(\"OPIK_PROJECT_NAME\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring and Tracing with Comet Opik\n",
    "\n",
    "Comet Opik is a great tool for monitoring and tracing LangGraph workflows. It is also available as open-source to run on your own infrastructure.\n",
    "\n",
    "Refer to https://www.comet.com/docs/opik/cookbook/langgraph for more details on how to use this with LangGraph.\n",
    "\n",
    "As the system becomes more and more complex, it's necessary to be able to monitor and trace the workflow. Otherwise it becomes a black box, an expensive black box!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Instructions\n",
    "Refer to the Maven page on how to setup Comet Opik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opik import Opik, track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opik Features Demo:\n",
    "\n",
    "Configure projects and workspaces in Comet Opik using the list here - https://www.comet.com/docs/opik/tracing/sdk_configuration#configuration-values.\n",
    "\n",
    "Opik also provides an SDK which lets you see the traces programmatically. Reference: https://www.comet.com/docs/opik/tracing/export_data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple LangGraph workflow which takes in a question and returns the category\n",
    "# of the question.\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define the state\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    category: str | None\n",
    "\n",
    "# Create the LLM\n",
    "llm = init_chat_model(model=\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n",
    "# Create the prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that categorizes questions amongst the following categories: \"\n",
    "              \"1. Math\"\n",
    "              \"2. Science\"\n",
    "              \"3. History\"\n",
    "              \"4. Geography\"\n",
    "              \"5. Other\"\n",
    "              \"Respond with just one word for the category.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Create the chain\n",
    "chain = prompt | llm\n",
    "\n",
    "# Define the processing function\n",
    "def process(state: State) -> State:\n",
    "    \"\"\"Process the question and determine its category.\"\"\"\n",
    "    result = chain.invoke({\"question\": state[\"question\"]})\n",
    "    return {\"question\": state[\"question\"], \"category\": result.content}\n",
    "\n",
    "# Create the graph\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Add the processing node\n",
    "workflow.add_node(\"process\", process)\n",
    "\n",
    "# Set the start and end points\n",
    "workflow.add_edge(START, \"process\")\n",
    "workflow.add_edge(\"process\", END)\n",
    "\n",
    "# Compile the graph\n",
    "graph = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the graph\n",
    "result = graph.invoke({\"question\": \"What is the capital of France?\"})\n",
    "print(f\"Question: {result['question']}\")\n",
    "print(f\"Category: {result['category']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opik.integrations.langchain import OpikTracer\n",
    "\n",
    "# Track the graph structure:\n",
    "tracer = OpikTracer(graph=graph.get_graph(xray=True))\n",
    "\n",
    "# Trace the invocation for a particular input:\n",
    "# This will generate a trace in the Comet Opik project.\n",
    "#\n",
    "# The use of tracer is optional for each invocation.\n",
    "result = graph.invoke({\"question\": \"Where is the coldest place on earth?\"}, config={\"callbacks\": [tracer]})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching through historical traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opik provides an API to search all the recorded traces within a project.\n",
    "# You can then use this information to generate your own metrics and monitor them if needed.\n",
    "# This will reduce the dependency on Comet Opik to implement everything.\n",
    "import opik\n",
    "client = opik.Opik()\n",
    "traces = client.search_traces(project_name=\"Cohort 2 Demo Project\", filter_string='')\n",
    "print(len(traces))\n",
    "print(traces[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric types and single data-point evaluation\n",
    "\n",
    "Comet Opik supports many metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opik.evaluation.metrics import Hallucination\n",
    "\n",
    "metric = Hallucination()\n",
    "\n",
    "metric.score(\n",
    "    input=\"What is the capital of France?\",\n",
    "    output=\"The capital of France is Paris. It is famous for its iconic Eiffel Tower and rich cultural heritage.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incorrect output should be marked with hallucination score close to 1.0\n",
    "metric.score(\n",
    "    input=\"What is the capital of France?\",\n",
    "    output=\"The capital of France is Italy, it has amazing food and wine!\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.comet.com/docs/opik/evaluation/metrics/g_eval\n",
    "# Uses Chain of Thought to evaluate the output\n",
    "from opik.evaluation.metrics import GEval\n",
    "\n",
    "metric = GEval(\n",
    "    task_introduction=\"You are a helpful judge tasked with evaluating output and compare it with EXPECTED_OUTPUT\",\n",
    "    evaluation_criteria=\"The OUTPUT must be very similar in semantic meaning and factual correctness to EXPECTED_OUTPUT\",\n",
    ")\n",
    "metric.score(\n",
    "    output=\"\"\"\n",
    "    OUTPUT: 16 eggs are left/remaining\n",
    "    EXPECTED_OUTPUT: 34 eggs are left/remaining\n",
    "    \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and Experiments\n",
    "\n",
    "Opik has a concept called Datasets. Datasets are a collection of inputs and expected outputs. These datasets can be combined with evaluation metrics to perform experiments on the entire dataset or a subset of it.\n",
    "\n",
    "Refer to https://www.comet.com/docs/opik/evaluation/overview for more details on how to create and use datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opik import Opik\n",
    "from opik.evaluation import evaluate\n",
    "\n",
    "# Get or create a dataset\n",
    "client = Opik()\n",
    "dataset = client.get_or_create_dataset(name=\"Cohort 2 - Assignment 1\")\n",
    "\n",
    "# Add dataset items to it from assignment 1:\n",
    "dataset.insert([\n",
    "    {\"input\": \"When did World War II end?\", \"expected_label\": \"history\"},\n",
    "    {\"input\": \"What is photosynthesis?\", \"expected_label\": \"science\"},\n",
    "    {\"input\": \"What is the capital of France?\", \"expected_label\": \"geography\"},\n",
    "    {\"input\": \"What is the value of 2 + 2?\", \"expected_label\": \"math\"},\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual Evaluation by iterating over the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple string matching metric\n",
    "from opik.evaluation.metrics import Equals\n",
    "\n",
    "metric = Equals()\n",
    "\n",
    "# Evaluate the langgraph agent using the dataset:\n",
    "def run_task(input_data):\n",
    "    return graph.invoke({\"question\": input_data})\n",
    "\n",
    "# Manually evaluate the dataset:\n",
    "for item in dataset.get_items():\n",
    "    data_input = item['input']\n",
    "    data_output = run_task(data_input)\n",
    "    print(metric.score(output=data_output['category'], reference=item['expected_label']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `evaluate` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opik.evaluation.metrics import Equals\n",
    "\n",
    "# Run the evaluation using `evaluate` function:\n",
    "def run_task(input_data):\n",
    "    result = graph.invoke({\"question\": input_data})\n",
    "    return {\"output\": result['category'], \"reference\": input_data['expected_label']}\n",
    "\n",
    "result = evaluate(\n",
    "    experiment_name=\"Cohort 2 - Assignment 1 Evaluation\",\n",
    "    dataset=dataset,\n",
    "    task=run_task,\n",
    "    scoring_metrics=[Equals()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
