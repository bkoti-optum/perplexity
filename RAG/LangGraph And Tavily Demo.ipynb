{"cells":[{"cell_type":"markdown","id":"39bbaed2","metadata":{"id":"39bbaed2"},"source":["## Assignment 2 Useful Concepts Walkthrough\n","This notebook covers the concepts that will be useful for solving the Assignment 2 problems:\n","1. Structured Outputs\n","2. LangGraph Basics\n","3. Tavily\n","4. How to do RAG in LangGraph"]},{"cell_type":"code","execution_count":null,"id":"42b6f264-addf-4a8e-a742-0d642a8b3b9d","metadata":{"id":"42b6f264-addf-4a8e-a742-0d642a8b3b9d"},"outputs":[],"source":["from dotenv import load_dotenv\n","\n","load_dotenv()\n","\n","import os\n","print(os.environ['TAVILY_API_KEY'][:20])\n","print(os.environ['OPENAI_API_KEY'][:20])"]},{"cell_type":"markdown","id":"29d2a90a","metadata":{"id":"29d2a90a"},"source":["### Structured Outputs\n","Force the LLM to return the output in a specific format (Pydantic class) instead of just a single text. This is extremely useful for integrating LLMs into your applications in a seamless way.\n","\n","Read more about that here - https://python.langchain.com/docs/concepts/structured_outputs/."]},{"cell_type":"code","execution_count":null,"id":"05f10297","metadata":{"id":"05f10297"},"outputs":[],"source":["from pydantic import BaseModel, Field\n","from langchain.chat_models import init_chat_model\n","from langchain_core.prompts import ChatPromptTemplate\n","\n","model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n","\n","class ClassificationResult(BaseModel):\n","    category: str = Field(description=\"The category of the user question\")\n","    explanation: str = Field(description=\"The explanation of why the category is chosen\")\n","\n","model = model.with_structured_output(ClassificationResult)\n","prompt = ChatPromptTemplate.from_template(\"\"\"\n","Take the user question and classify it into one of the following categories:\n","Geography: Questions about countries, cities, or geographical features.\n","Science: Questions about science, technology, or scientific concepts.\n","Math: Questions about math, algebra, geometry, or mathematical concepts.\n","\"\"\")\n","chain = prompt | model\n","result = chain.invoke({\"question\": \"What is the capital of France?\"})\n","print(type(result))\n","print(result)\n","\n","# Without structured output, LLM will return only the text.\n","plain_model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n","model_only_result = plain_model.invoke(\"What is the capital of France?\")\n","print(type(model_only_result))\n","print(model_only_result)"]},{"cell_type":"markdown","id":"c3038804","metadata":{"id":"c3038804"},"source":["Structured output can also work with more complex data structures like lists as well. See the below example for identifying all places user asks in a question."]},{"cell_type":"code","execution_count":null,"id":"08c8fcbe","metadata":{"id":"08c8fcbe"},"outputs":[],"source":["from pydantic import BaseModel, Field\n","\n","class Places(BaseModel):\n","    places: list[str] = Field(description=\"List of places\")\n","\n","model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n","\n","model = model.with_structured_output(Places)\n","\n","# NOTE: This example is not answering the question, just extracting information\n","# from the question. You can also use it to answer the question.\n","prompt = ChatPromptTemplate.from_template(\"\"\"\n","List all the places that the user asked in the question: {question}\n","\"\"\")\n","\n","chain = prompt | model\n","result = chain.invoke({\"question\": \"What is the capital of France and India?\"})\n","print(type(result))\n","print(result)"]},{"cell_type":"markdown","id":"7b8f0e64","metadata":{"id":"7b8f0e64"},"source":["## Simple non-AI graph example"]},{"cell_type":"code","execution_count":null,"id":"15bdfe1d","metadata":{"id":"15bdfe1d"},"outputs":[],"source":["# Define the state of the graph:\n","from typing import TypedDict\n","\n","class State(TypedDict):\n","    num_values: int\n","    generated_values: list[int]\n","    result: int | None = None\n"]},{"cell_type":"code","execution_count":null,"id":"674d9e72","metadata":{"id":"674d9e72"},"outputs":[],"source":["import random\n","\n","# Define the nodes in the graph:\n","def generate_values(state: State):\n","    values = [random.randint(0, 10) for _ in range(state[\"num_values\"])]\n","    return {\"generated_values\": values}\n","\n","def add(state: State):\n","    result = sum(state[\"generated_values\"])\n","    return {\"result\": result}"]},{"cell_type":"code","execution_count":null,"id":"f7561b6b","metadata":{"id":"f7561b6b"},"outputs":[],"source":["# Define the graph:\n","from langgraph.graph import StateGraph\n","from langgraph.graph import START, END\n","\n","graph_builder = StateGraph(State)\n","graph_builder.add_node(\"generate_values\", generate_values)\n","graph_builder.add_node(\"add\", add)\n","\n","graph_builder.add_edge(START, \"generate_values\")\n","graph_builder.add_edge(\"generate_values\", \"add\")\n","graph_builder.add_edge(\"add\", END)\n","\n","# Now graph is also a 'Runnable'\n","graph = graph_builder.compile()"]},{"cell_type":"code","execution_count":null,"id":"0243654c","metadata":{"id":"0243654c"},"outputs":[],"source":["from IPython.display import Image, display\n","\n","display(Image(graph.get_graph().draw_mermaid_png()))"]},{"cell_type":"code","execution_count":null,"id":"6a44f409","metadata":{"id":"6a44f409"},"outputs":[],"source":["graph.invoke({\"num_values\": 20})"]},{"cell_type":"markdown","id":"9605a434","metadata":{"id":"9605a434"},"source":["## Conditional Edge example in LangGraph"]},{"cell_type":"code","execution_count":null,"id":"792f7ef1","metadata":{"id":"792f7ef1"},"outputs":[],"source":["class ConditionalEdgeState(TypedDict):\n","    num_values: int\n","    generated_values: list[int]\n","    total: int\n","    # Prints happy if the total is even, sad if the total is odd\n","    final_message: str\n","\n","def generate_values(state: ConditionalEdgeState):\n","    generated_values = [random.randint(0, 10) for _ in range(state[\"num_values\"])]\n","    return {\"generated_values\": generated_values}\n","\n","def add(state: ConditionalEdgeState):\n","    total = sum(state[\"generated_values\"])\n","    return {\"total\": total}\n","\n","def check_total(state: ConditionalEdgeState):\n","    if state[\"total\"] % 2 == 0:\n","        return \"happy_message\"\n","    else:\n","        return \"sad_message\"\n","\n","def happy_message(state: ConditionalEdgeState):\n","    return {\"final_message\": \"happy\"}\n","\n","def sad_message(state: ConditionalEdgeState):\n","    return {\"final_message\": \"sad\"}\n","\n","graph_builder = StateGraph(ConditionalEdgeState)\n","\n","graph_builder.add_node(\"generate_values\", generate_values)\n","graph_builder.add_node(\"add\", add)\n","# graph_builder.add_node(\"check_total\", check_total)\n","graph_builder.add_node(\"happy_message\", happy_message)\n","graph_builder.add_node(\"sad_message\", sad_message)\n","\n","graph_builder.add_edge(START, \"generate_values\")\n","graph_builder.add_edge(\"generate_values\", \"add\")\n","# THIS is the place where the condition is evaluated\n","graph_builder.add_conditional_edges(\n","    \"add\",\n","    check_total,\n","    {\n","        \"happy_message\": \"happy_message\",\n","        \"sad_message\": \"sad_message\",\n","    },\n",")\n","graph_builder.add_edge(\"happy_message\", END)\n","graph_builder.add_edge(\"sad_message\", END)\n","\n","graph = graph_builder.compile()\n"]},{"cell_type":"code","execution_count":null,"id":"dc2bd4bf","metadata":{"id":"dc2bd4bf"},"outputs":[],"source":["# Display the graph\n","display(Image(graph.get_graph().draw_mermaid_png()))"]},{"cell_type":"code","execution_count":null,"id":"b28c2629","metadata":{"id":"b28c2629"},"outputs":[],"source":["graph.invoke({\"num_values\": 10})"]},{"cell_type":"markdown","id":"89319771","metadata":{"id":"89319771"},"source":["## Tavily Web Search Example"]},{"cell_type":"code","execution_count":null,"id":"fb2a01fd","metadata":{"id":"fb2a01fd"},"outputs":[],"source":["from langchain_community.tools import TavilySearchResults\n","\n","tool = TavilySearchResults(\n","    max_results=5,\n","    include_answer=True,\n","    include_raw_content=True,\n","    include_images=False,\n","    search_depth=\"advanced\",\n","    # include_domains = []\n","    # exclude_domains = []\n",")"]},{"cell_type":"code","execution_count":null,"id":"60dd07e3","metadata":{"id":"60dd07e3"},"outputs":[],"source":["# Calling the tool directly\n","tool.invoke({\"query\": \"What are some latest innovations in AI?\"})"]},{"cell_type":"code","execution_count":null,"id":"c4d2d5ea","metadata":{"id":"c4d2d5ea"},"outputs":[],"source":["class WebSearchState(TypedDict):\n","    query: str\n","    results: list[dict]\n","    char_counts: list[int]\n","\n","def web_search(state: WebSearchState):\n","    results = tool.invoke({\"query\": state[\"query\"]})\n","    return {\"results\": results}\n","\n","def sum_char_counts(state: WebSearchState):\n","    # Count length of each result:\n","    char_counts = [len(result[\"content\"]) for result in state[\"results\"]]\n","    return {\"char_counts\": char_counts}\n","\n","graph_builder = StateGraph(WebSearchState)\n","\n","graph_builder.add_node(\"web_search\", web_search)\n","graph_builder.add_node(\"sum_char_counts\", sum_char_counts)\n","\n","graph_builder.add_edge(START, \"web_search\")\n","graph_builder.add_edge(\"web_search\", \"sum_char_counts\")\n","graph_builder.add_edge(\"sum_char_counts\", END)\n","\n","graph = graph_builder.compile()"]},{"cell_type":"code","execution_count":null,"id":"22688d2c","metadata":{"id":"22688d2c"},"outputs":[],"source":["# Display the graph\n","display(Image(graph.get_graph().draw_mermaid_png()))"]},{"cell_type":"code","execution_count":null,"id":"327c9754","metadata":{"id":"327c9754"},"outputs":[],"source":["graph.invoke({\"query\": \"What are some latest innovations in AI?\"})"]},{"cell_type":"code","execution_count":null,"id":"d42e7a96","metadata":{"id":"d42e7a96"},"outputs":[],"source":["# Lets implement the above in LangChain instead of LangGraph to cross-check your understanding:\n","from langchain_core.runnables import RunnableLambda\n","from langchain_community.tools import TavilySearchResults\n","\n","search_tool = TavilySearchResults(\n","    max_results=5,\n","    include_answer=True,\n","    include_raw_content=True,\n","    include_images=False,\n","    search_depth=\"advanced\",\n","    # include_domains = []\n","    # exclude_domains = []\n",")\n","\n","# Define the character counting function\n","def sum_char_counts(results: dict) -> dict:\n","    print(results)\n","    char_counts = [len(result[\"content\"]) for result in results]\n","    return {\"char_counts\": char_counts}\n","\n","# Create the chain using RunnableLambda\n","chain = search_tool | RunnableLambda(sum_char_counts)\n","\n","# Example usage:\n","result = chain.invoke({\"query\": \"What are some latest innovations in AI?\"})\n","print(result)"]},{"cell_type":"markdown","id":"72f898cd","metadata":{"id":"72f898cd"},"source":["## Why LangGraph then where I can do everything in LangChain?\n","- THE central framework from LangChain company.\n","- Less opinionated, more flexibility.\n","- Can still use LangChain within the nodes of a graph.\n","- Highly useful for Agentic AI which you'll be building next week."]},{"cell_type":"markdown","id":"08af068c","metadata":{"id":"08af068c"},"source":["## RAG Example in LangGraph"]},{"cell_type":"code","execution_count":null,"id":"ccd719c7","metadata":{"id":"ccd719c7"},"outputs":[],"source":["# Example of loading a PDF file:\n","# Refer here for more examples of document loaders: https://python.langchain.com/docs/how_to/document_loader_pdf/\n","from langchain_community.document_loaders import PyPDFLoader\n","\n","file_path = \"/Users/aish/Downloads/2019-annual-performance-report.pdf\"\n","loader = PyPDFLoader(file_path)\n","pages = []\n","async for page in loader.alazy_load():\n","    pages.append(page)"]},{"cell_type":"code","execution_count":null,"id":"690ef2ea","metadata":{"id":"690ef2ea"},"outputs":[],"source":["print(len(pages))\n","print(pages[10].page_content)"]},{"cell_type":"markdown","id":"490b28c5","metadata":{"id":"490b28c5"},"source":["## Indexing the PDF file into a vector store"]},{"cell_type":"code","execution_count":null,"id":"8a090361","metadata":{"id":"8a090361"},"outputs":[],"source":["from langchain_core.vectorstores import InMemoryVectorStore\n","from langchain_openai import OpenAIEmbeddings\n","\n","vector_store = InMemoryVectorStore.from_documents(pages, OpenAIEmbeddings())\n","docs = vector_store.similarity_search(\"What are OPM's goals?\", k=4)\n","print(len(docs))\n","for doc in docs:\n","    print(f'Page {doc.metadata[\"page\"]}: {doc.page_content[:300]}\\n')"]},{"cell_type":"markdown","id":"bd9f2b00","metadata":{"id":"bd9f2b00"},"source":["## Retrieval in RAG"]},{"cell_type":"code","execution_count":null,"id":"d638c09e","metadata":{"id":"d638c09e"},"outputs":[],"source":["from typing import TypedDict\n","\n","# Create a retriever object on the vector store\n","retriever = vector_store.as_retriever()\n","\n","from langchain.chat_models import init_chat_model\n","from langchain_core.prompts import PromptTemplate\n","\n","model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n","\n","class GraphState(TypedDict):\n","    question: str\n","    documents: list[str]\n","    generated_answer: str\n","\n","def retrieve_documents(state: GraphState):\n","    docs = retriever.get_relevant_documents(state[\"question\"])\n","    return {\"documents\": docs}\n","\n","def generate_answer(state: GraphState):\n","    # Call the model to generate the answer using the retrieved documents\n","    # and the question\n","    # Join all the retrieved documents into a single string\n","    prompt = PromptTemplate.from_template(\n","        \"Answer the question based on the context provided. \\n\"\n","        \"Question: {question}\\n\"\n","        \"Context: {context}\\n\"\n","        \"Answer: \"\n","    )\n","    context = \"\\n\".join([doc.page_content for doc in state[\"documents\"]])\n","    prompt = prompt.format(question=state[\"question\"], context=context)\n","    answer = model.invoke(prompt).content\n","    return {\"generated_answer\": answer}\n","\n","graph_builder = StateGraph(GraphState)\n","\n","graph_builder.add_node(\"retrieve_documents\", retrieve_documents)\n","graph_builder.add_node(\"generate_answer\", generate_answer)\n","\n","graph_builder.add_edge(START, \"retrieve_documents\")\n","graph_builder.add_edge(\"retrieve_documents\", \"generate_answer\")\n","graph_builder.add_edge(\"generate_answer\", END)\n","\n","graph = graph_builder.compile()"]},{"cell_type":"code","execution_count":null,"id":"ce037e40","metadata":{"id":"ce037e40"},"outputs":[],"source":["# Display the graph\n","display(Image(graph.get_graph().draw_mermaid_png()))"]},{"cell_type":"code","execution_count":null,"id":"25c1309b","metadata":{"id":"25c1309b"},"outputs":[],"source":["graph.invoke({\"question\": \"What are OPM's goals?\"})"]},{"cell_type":"code","execution_count":null,"id":"06b42850","metadata":{"id":"06b42850"},"outputs":[],"source":["graph.invoke({\"question\": \"Who won the french open today?\"})"]},{"cell_type":"code","execution_count":null,"id":"01b6d8af","metadata":{"id":"01b6d8af"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}